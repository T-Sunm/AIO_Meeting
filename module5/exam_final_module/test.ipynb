{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional\n",
    "\n",
    "# Create two tensors\n",
    "# Tensor a - shape (2,): 1D tensor with 2 elements\n",
    "a = torch.tensor([0, 1])\n",
    "\n",
    "# Tensor b - shape (2, 3): 2D tensor with 2 rows and 3 columns\n",
    "b = torch.tensor([[1, 2, 3],\n",
    "                  [4, 5, 6]])\n",
    "\n",
    "# Matrix multiplication between tensors a and b\n",
    "h = torch.matmul(a, b)\n",
    "h = torch.nn.functional.relu(h)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer output (h): tensor([[3.2000, 0.0000]])\n",
      "Output layer logits (z): tensor([[0.9600, 0.6400, 0.3200]])\n",
      "Loss: 0.8124586343765259\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Input vector X\n",
    "X = torch.tensor([[3.0, 2.0]])\n",
    "\n",
    "# Weights and biases for the hidden layer\n",
    "W_h = torch.tensor([[0.8, -1.0],\n",
    "                    [0.4, -0.6]])\n",
    "b_h = torch.tensor([[0.0, 0.0]])\n",
    "\n",
    "# Weights and biases for the output layer\n",
    "W_z = torch.tensor([[0.3, 0.2, 0.1],\n",
    "                    [-0.4, -1.0, 0.2]])\n",
    "b_z = torch.tensor([[0.0, 0.0, 0.0]])\n",
    "\n",
    "# Target (ground truth) label\n",
    "y = torch.tensor([0])  # Class 0 is the correct label\n",
    "\n",
    "# Forward pass\n",
    "# 1. Hidden layer computation\n",
    "h = torch.matmul(X, W_h) + b_h\n",
    "h = F.relu(h)  # Apply ReLU activation\n",
    "\n",
    "# 2. Output layer computation\n",
    "z = torch.matmul(h, W_z) + b_z\n",
    "\n",
    "# 3. Apply softmax (implicitly handled by CrossEntropyLoss)\n",
    "# CrossEntropyLoss expects raw logits, not softmax probabilities\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(z, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Hidden layer output (h):\", h)\n",
    "print(\"Output layer logits (z):\", z)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits (z): tensor([[ 0.3585, -0.0033,  0.2296]], grad_fn=<AddmmBackward0>)\n",
      "Loss: 0.9460580348968506\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "# Define the model using nn.Linear and nn.ReLU\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        \n",
    "        # Define the hidden layer (2 input features to 2 hidden units)\n",
    "        self.fc1 = nn.Linear(2, 2)  # Hidden layer with 2 inputs and 2 outputs\n",
    "        self.fc1.weight.data = torch.randn(2, 2)  # Random weights for hidden layer\n",
    "        self.fc1.bias.data = torch.randn(2)  # Random biases for hidden layer\n",
    "\n",
    "        # Define the output layer (2 hidden units to 3 output classes)\n",
    "        self.fc2 = nn.Linear(2, 3)  # Output layer with 2 inputs and 3 outputs\n",
    "        self.fc2.weight.data = torch.tensor([[0.3, -0.4],\n",
    "                                             [0.2, -1.0],\n",
    "                                             [0.1, 0.2]])  # Fixed weights for output layer\n",
    "        self.fc2.bias.data = torch.tensor([0.0, 0.0, 0.0])  # Fixed biases for output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the hidden layer with ReLU activation\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "\n",
    "        # Forward pass through the output layer\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Input data\n",
    "X = torch.tensor([[3.0, 2.0]])  # Input vector\n",
    "y = torch.tensor([0])  # Target label\n",
    "\n",
    "# Forward pass\n",
    "z = model(X)\n",
    "\n",
    "# Compute loss\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss = loss_fn(z, y)\n",
    "\n",
    "# Print results\n",
    "print(\"Logits (z):\", z)\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
