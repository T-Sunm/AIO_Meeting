{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dataset_name = \"facades\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !bash download_pix2pix_dataset.sh facades "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision  \n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None, split=\"train\"):\n",
    "        self.transform = torchvision.transforms.Compose(transforms)\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, split) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h))\n",
    "        img_B = img.crop((w / 2, 0, w, h))\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "\n",
    "        img_A = self.transform(img_A)\n",
    "        img_B = self.transform(img_B)\n",
    "\n",
    "        return {\"A\": img_A, \"B\": img_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "transforms = [\n",
    "    torchvision.transforms.Resize((img_height, img_width), Image.BICUBIC),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "train_set = ImageDataset(root=f\"data/{dataset_name}\", transforms=transforms, split=\"train\")\n",
    "val_set = ImageDataset(root=f\"data/{dataset_name}\", transforms=transforms, split=\"val\")\n",
    "\n",
    "len(train_set), len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()   \n",
    "        self.model = nn.Sequential(nn.Conv2d(in_channels, out_channels, \n",
    "                                             kernel_size=4, stride=2, \n",
    "                                             padding=1, bias=False),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.ConvTranspose2d(in_channels, out_channels, \n",
    "                                                      kernel_size=4, stride=2, \n",
    "                                                      padding=1, bias=False),\n",
    "                                   nn.LeakyReLU(0.2))\n",
    "\n",
    "    def forward(self, x, skip_input):\n",
    "        x = self.model(x)\n",
    "        x = torch.cat((x, skip_input), 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3):\n",
    "        super().__init__()\n",
    "\n",
    "        self.down1 = DownSample(in_channels, 64)\n",
    "        self.down2 = DownSample(64, 128)\n",
    "        self.down3 = DownSample(128, 256)\n",
    "        self.down4 = DownSample(256, 512)\n",
    "        self.down5 = DownSample(512, 512)\n",
    "        self.down6 = DownSample(512, 512)\n",
    "        self.down7 = DownSample(512, 512)\n",
    "        self.down8 = DownSample(512, 512)\n",
    "\n",
    "        self.up1 = UpSample(512, 512)\n",
    "        self.up2 = UpSample(1024, 512)\n",
    "        self.up3 = UpSample(1024, 512)\n",
    "        self.up4 = UpSample(1024, 512)\n",
    "        self.up5 = UpSample(1024, 256)\n",
    "        self.up6 = UpSample(512, 128)\n",
    "        self.up7 = UpSample(256, 64)\n",
    "\n",
    "        self.final = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(128, out_channels, kernel_size=4, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        d1 = self.down1(x)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        d4 = self.down4(d3)\n",
    "        d5 = self.down5(d4)\n",
    "        d6 = self.down6(d5)\n",
    "        d7 = self.down7(d6)\n",
    "        d8 = self.down8(d7)\n",
    "        u1 = self.up1(d8, d7)\n",
    "        u2 = self.up2(u1, d6)\n",
    "        u3 = self.up3(u2, d5)\n",
    "        u4 = self.up4(u3, d4)\n",
    "        u5 = self.up5(u4, d3)\n",
    "        u6 = self.up6(u5, d2)\n",
    "        u7 = self.up7(u6, d1)\n",
    "        return self.final(u7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.InstanceNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.InstanceNorm2d(256),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.InstanceNorm2d(512),\n",
    "\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, kernel_size=4, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, img_A, img_B):\n",
    "        img_input = torch.cat((img_A, img_B), 1)\n",
    "        return self.model(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (down1): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down2): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down3): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down4): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down5): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down6): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down7): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (down8): DownSample(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up1): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up2): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up3): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up4): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up5): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(1024, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up6): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(512, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (up7): UpSample(\n",
       "    (model): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (1): LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "  )\n",
       "  (final): Sequential(\n",
       "    (0): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (1): ZeroPad2d((1, 0, 1, 0))\n",
       "    (2): Conv2d(128, 3, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (3): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (7): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (10): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (11): ZeroPad2d((1, 0, 1, 0))\n",
       "    (12): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (13): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discriminator = Discriminator()\n",
    "discriminator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_images(epoch_done):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs = next(iter(val_loader))\n",
    "    real_A = imgs[\"B\"][:5, ...].to(device)\n",
    "    real_B = imgs[\"A\"][:5, ...].to(device)\n",
    "    fake_B = generator(real_A)\n",
    "    img_sample = torch.cat((real_A.data, fake_B.data, real_B.data), -2)\n",
    "    save_image(img_sample, f\"images/epoch_{epoch_done}.png\", nrow=5, normalize=True)\n",
    "\n",
    "save_interval = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200], Train G Loss: 41.4839, Train D Loss: 0.7295\n",
      "Epoch [2/200], Train G Loss: 40.2135, Train D Loss: 0.5352\n",
      "Epoch [3/200], Train G Loss: 39.5436, Train D Loss: 0.4103\n",
      "Epoch [4/200], Train G Loss: 38.4869, Train D Loss: 0.4579\n",
      "Epoch [5/200], Train G Loss: 38.3749, Train D Loss: 0.4356\n",
      "Epoch [6/200], Train G Loss: 38.1018, Train D Loss: 0.3581\n",
      "Epoch [7/200], Train G Loss: 39.0077, Train D Loss: 0.4121\n",
      "Epoch [8/200], Train G Loss: 39.1658, Train D Loss: 0.2685\n",
      "Epoch [9/200], Train G Loss: 38.7094, Train D Loss: 0.2905\n",
      "Epoch [10/200], Train G Loss: 38.7604, Train D Loss: 0.3387\n",
      "Epoch [11/200], Train G Loss: 38.2559, Train D Loss: 0.2699\n",
      "Epoch [12/200], Train G Loss: 39.6258, Train D Loss: 0.3807\n",
      "Epoch [13/200], Train G Loss: 37.9199, Train D Loss: 0.3039\n",
      "Epoch [14/200], Train G Loss: 39.0829, Train D Loss: 0.2919\n",
      "Epoch [15/200], Train G Loss: 37.7095, Train D Loss: 0.2977\n",
      "Epoch [16/200], Train G Loss: 38.6199, Train D Loss: 0.1993\n",
      "Epoch [17/200], Train G Loss: 38.2996, Train D Loss: 0.3261\n",
      "Epoch [18/200], Train G Loss: 38.3814, Train D Loss: 0.2981\n",
      "Epoch [19/200], Train G Loss: 38.5810, Train D Loss: 0.2383\n",
      "Epoch [20/200], Train G Loss: 38.7734, Train D Loss: 0.2745\n",
      "Epoch [21/200], Train G Loss: 38.4082, Train D Loss: 0.2273\n",
      "Epoch [22/200], Train G Loss: 38.4541, Train D Loss: 0.2860\n",
      "Epoch [23/200], Train G Loss: 39.1954, Train D Loss: 0.3519\n",
      "Epoch [24/200], Train G Loss: 37.6457, Train D Loss: 0.2259\n",
      "Epoch [25/200], Train G Loss: 37.9662, Train D Loss: 0.1557\n",
      "Epoch [26/200], Train G Loss: 38.9996, Train D Loss: 0.2158\n",
      "Epoch [27/200], Train G Loss: 38.2241, Train D Loss: 0.1513\n",
      "Epoch [28/200], Train G Loss: 40.0490, Train D Loss: 0.3321\n",
      "Epoch [29/200], Train G Loss: 39.2974, Train D Loss: 0.2763\n",
      "Epoch [30/200], Train G Loss: 39.4314, Train D Loss: 0.1848\n",
      "Epoch [31/200], Train G Loss: 39.4004, Train D Loss: 0.1450\n",
      "Epoch [32/200], Train G Loss: 38.6846, Train D Loss: 0.1849\n",
      "Epoch [33/200], Train G Loss: 38.4368, Train D Loss: 0.4162\n",
      "Epoch [34/200], Train G Loss: 38.4161, Train D Loss: 0.2260\n",
      "Epoch [35/200], Train G Loss: 37.8456, Train D Loss: 0.1226\n",
      "Epoch [36/200], Train G Loss: 38.6532, Train D Loss: 0.1215\n",
      "Epoch [37/200], Train G Loss: 37.5895, Train D Loss: 0.2620\n",
      "Epoch [38/200], Train G Loss: 38.8977, Train D Loss: 0.2194\n",
      "Epoch [39/200], Train G Loss: 38.6294, Train D Loss: 0.1119\n",
      "Epoch [40/200], Train G Loss: 37.5816, Train D Loss: 0.1746\n",
      "Epoch [41/200], Train G Loss: 38.8100, Train D Loss: 0.3433\n",
      "Epoch [42/200], Train G Loss: 38.9407, Train D Loss: 0.1891\n",
      "Epoch [43/200], Train G Loss: 37.7491, Train D Loss: 0.1693\n",
      "Epoch [44/200], Train G Loss: 37.9512, Train D Loss: 0.1722\n",
      "Epoch [45/200], Train G Loss: 38.0746, Train D Loss: 0.2867\n",
      "Epoch [46/200], Train G Loss: 37.8562, Train D Loss: 0.1591\n",
      "Epoch [47/200], Train G Loss: 36.5838, Train D Loss: 0.1443\n",
      "Epoch [48/200], Train G Loss: 36.6481, Train D Loss: 0.1669\n",
      "Epoch [49/200], Train G Loss: 36.2805, Train D Loss: 0.2333\n",
      "Epoch [50/200], Train G Loss: 36.5863, Train D Loss: 0.3436\n",
      "Epoch [51/200], Train G Loss: 37.7291, Train D Loss: 0.3416\n",
      "Epoch [52/200], Train G Loss: 36.3798, Train D Loss: 0.2703\n",
      "Epoch [53/200], Train G Loss: 35.8768, Train D Loss: 0.2314\n",
      "Epoch [54/200], Train G Loss: 36.2391, Train D Loss: 0.2296\n",
      "Epoch [55/200], Train G Loss: 36.6609, Train D Loss: 0.1952\n",
      "Epoch [56/200], Train G Loss: 37.0735, Train D Loss: 0.2064\n",
      "Epoch [57/200], Train G Loss: 37.2024, Train D Loss: 0.2517\n",
      "Epoch [58/200], Train G Loss: 36.9281, Train D Loss: 0.2011\n",
      "Epoch [59/200], Train G Loss: 37.0194, Train D Loss: 0.1122\n",
      "Epoch [60/200], Train G Loss: 36.4188, Train D Loss: 0.1911\n",
      "Epoch [61/200], Train G Loss: 36.2573, Train D Loss: 0.2286\n",
      "Epoch [62/200], Train G Loss: 34.8311, Train D Loss: 0.1203\n",
      "Epoch [63/200], Train G Loss: 34.5704, Train D Loss: 0.1785\n",
      "Epoch [64/200], Train G Loss: 34.6441, Train D Loss: 0.2331\n",
      "Epoch [65/200], Train G Loss: 35.2207, Train D Loss: 0.2113\n",
      "Epoch [66/200], Train G Loss: 35.2149, Train D Loss: 0.2911\n",
      "Epoch [67/200], Train G Loss: 35.2270, Train D Loss: 0.2952\n",
      "Epoch [68/200], Train G Loss: 35.3916, Train D Loss: 0.2250\n",
      "Epoch [69/200], Train G Loss: 34.9131, Train D Loss: 0.2160\n",
      "Epoch [70/200], Train G Loss: 34.1799, Train D Loss: 0.1678\n",
      "Epoch [71/200], Train G Loss: 33.0200, Train D Loss: 0.1703\n",
      "Epoch [72/200], Train G Loss: 33.3714, Train D Loss: 0.1972\n",
      "Epoch [73/200], Train G Loss: 34.4522, Train D Loss: 0.2348\n",
      "Epoch [74/200], Train G Loss: 35.4374, Train D Loss: 0.2122\n",
      "Epoch [75/200], Train G Loss: 34.5910, Train D Loss: 0.1702\n",
      "Epoch [76/200], Train G Loss: 33.8782, Train D Loss: 0.2006\n",
      "Epoch [77/200], Train G Loss: 33.4146, Train D Loss: 0.2589\n",
      "Epoch [78/200], Train G Loss: 33.8176, Train D Loss: 0.2572\n",
      "Epoch [79/200], Train G Loss: 34.1095, Train D Loss: 0.2034\n",
      "Epoch [80/200], Train G Loss: 34.3533, Train D Loss: 0.2223\n",
      "Epoch [81/200], Train G Loss: 35.0083, Train D Loss: 0.2004\n",
      "Epoch [82/200], Train G Loss: 33.7464, Train D Loss: 0.1195\n",
      "Epoch [83/200], Train G Loss: 31.9394, Train D Loss: 0.1858\n",
      "Epoch [84/200], Train G Loss: 31.7589, Train D Loss: 0.4300\n",
      "Epoch [85/200], Train G Loss: 32.7941, Train D Loss: 0.3151\n",
      "Epoch [86/200], Train G Loss: 34.1295, Train D Loss: 0.1086\n",
      "Epoch [87/200], Train G Loss: 32.5011, Train D Loss: 0.1300\n",
      "Epoch [88/200], Train G Loss: 31.1419, Train D Loss: 0.2280\n",
      "Epoch [89/200], Train G Loss: 31.4160, Train D Loss: 0.2524\n",
      "Epoch [90/200], Train G Loss: 31.7538, Train D Loss: 0.2241\n",
      "Epoch [91/200], Train G Loss: 32.3848, Train D Loss: 0.2148\n",
      "Epoch [92/200], Train G Loss: 32.0166, Train D Loss: 0.0909\n",
      "Epoch [93/200], Train G Loss: 31.5405, Train D Loss: 0.0776\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 200\n",
    "\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0001)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "\n",
    "criterion = torch.nn.BCELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()\n",
    "\n",
    "lambda_pixel = 100\n",
    "patch = (1, img_height // 2 ** 4, img_width // 2 ** 4)\n",
    "\n",
    "hist = {\n",
    "        \"train_G_loss\": [],\n",
    "        \"train_D_loss\": [],\n",
    "    }\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_G_loss = 0.0\n",
    "    running_D_loss = 0.0\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        \n",
    "        real_imgs_A = batch[\"B\"].to(device)\n",
    "        real_imgs_B = batch[\"A\"].to(device)\n",
    "\n",
    "        valid = torch.ones(real_imgs_A.size(0), *patch).to(device)\n",
    "        fake = torch.zeros(real_imgs_A.size(0), *patch).to(device)\n",
    "\n",
    "        # --- Train Generator ---\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        fake_imgs_B = generator(real_imgs_A)\n",
    "        pred_fake = discriminator(fake_imgs_B, real_imgs_A)\n",
    "\n",
    "        # GAN loss\n",
    "        G_loss_GAN = criterion(pred_fake, valid)\n",
    "\n",
    "        # Pixel-wise loss\n",
    "        G_loss_pixel = criterion_pixelwise(fake_imgs_B, real_imgs_B)\n",
    "\n",
    "        # Total loss\n",
    "        G_loss = G_loss_GAN + lambda_pixel * G_loss_pixel\n",
    "\n",
    "        running_G_loss += G_loss.item()\n",
    "        G_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # --- Train Discriminator ---\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        pred_real = discriminator(real_imgs_B, real_imgs_A)\n",
    "        loss_real = criterion(pred_real, valid)\n",
    "\n",
    "        pred_fake = discriminator(fake_imgs_B.detach(), real_imgs_A)\n",
    "        loss_fake = criterion(pred_fake, fake)\n",
    "\n",
    "        # Total loss\n",
    "        D_loss = (loss_real + loss_fake) / 2\n",
    "\n",
    "        running_D_loss += D_loss.item()\n",
    "        D_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "    \n",
    "    epoch_G_loss = running_G_loss / len(train_loader)\n",
    "    epoch_D_loss = running_D_loss / len(train_loader)\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{EPOCHS}], Train G Loss: {epoch_G_loss:.4f}, Train D Loss: {epoch_D_loss:.4f}\")\n",
    "\n",
    "    hist[\"train_G_loss\"].append(epoch_G_loss)\n",
    "    hist[\"train_D_loss\"].append(epoch_D_loss)\n",
    "\n",
    "    if epoch % save_interval == 0:\n",
    "        sample_images(epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
